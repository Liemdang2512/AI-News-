import asyncio
from typing import List, Optional, Callable, Awaitable, AsyncGenerator, Dict
from datetime import datetime
from bs4 import BeautifulSoup
from services.secure_fetcher import secure_fetcher
from services.rss_fetcher import rss_fetcher
from services.gemini_client import gemini_client
from prompts import SINGLE_ARTICLE_SUMMARIZE_PROMPT

class Summarizer:
    """
    Fetches article content and generates AI summaries
    Refactored to process articles in parallel for better performance and reliability.
    """
    
    def __init__(self):
        # Semaphore to limit concurrent tasks (avoid rate limits)
        self.semaphore = asyncio.Semaphore(10) # Process 10 articles at a time
        
    async def summarize_articles_generator(
        self, 
        urls: List[str], 
        api_key: str = None, 
        articles_metadata: dict = None
    ) -> AsyncGenerator[Dict, None]:
        """
        Summarize articles and yield progress updates (for StreamingResponse)
        Yields dicts: {'type': 'progress', ...} or {'type': 'complete', 'summary': ...}
        """
        if not urls:
            yield {"type": "error", "message": "No articles selected"}
            return

        if not articles_metadata:
            articles_metadata = {}

        tasks = []
        total = len(urls)
        completed = 0
        
        # We need to process tasks and yield as they complete
        # Create a list of coroutines
        for url in urls:
            clean_url = url.strip().rstrip('/')
            metadata = articles_metadata.get(clean_url, {})
            if not metadata:
                metadata = articles_metadata.get(url, {})
            
            # Fallback logic
            if not metadata or not metadata.get('category'):
                inferred_category = rss_fetcher._extract_category_from_url(url)
                if not metadata: metadata = {'source': None, 'title': None}
                metadata['category'] = inferred_category

            tasks.append(self._process_single_article(url, metadata, api_key))
            
        # Process in batches (BATCH_SIZE = 5) and yield progress
        BATCH_SIZE = 5
        all_results = []
        
        for i in range(0, len(tasks), BATCH_SIZE):
            batch_tasks = tasks[i:i + BATCH_SIZE]
            batch_urls = urls[i:i + BATCH_SIZE]
            
            # Current batch progress update
            for url in batch_urls:
                 # Try to get title for better UI
                 meta = articles_metadata.get(url.strip().rstrip('/')) or articles_metadata.get(url) or {}
                 title = meta.get('title', url)
                 yield {
                     "type": "progress",
                     "completed": completed,
                     "total": total,
                     "current_article": f"ƒêang x·ª≠ l√Ω: {title}...",
                     "status": "processing"
                 }
            
            batch_results = await asyncio.gather(*batch_tasks)
            all_results.extend(batch_results)
            completed += len(batch_results)
            
            # Update progress after batch
            yield {
                "type": "progress",
                "completed": completed,
                "total": total,
                "current_article": "ƒêang chuy·ªÉn sang l√¥ ti·∫øp theo...",
                "status": "processing"
            }
            
            if i + BATCH_SIZE < len(tasks):
                await asyncio.sleep(2) 
                
        # ... (rest of processing logic identical to summarize_articles) ...
        # Copied logic for combining results
        
        results = all_results
        
        # Sort results by category
        # ... (reuse logic) ...
        # For brevity, calling internal helper or duplicating logic? 
        # Duplicating logic is safer to avoid breaking existing method
        
        categorized_results = {
            "KINH T·∫æ": [],
            "T√ÄI CH√çNH": [],
            "X√É H·ªòI": [],
            "PH√ÅP LU·∫¨T": [],
            "TH·∫æ GI·ªöI": [],
            "KH√ÅC": []
        }
        
        failed_articles = []
        
        for res in results:
            if "error" in res or "text" not in res:
                 # Check if it was a fallback text result (which is a dict with category/text)
                 if isinstance(res, dict) and "text" in res and "category" in res:
                     # It's a valid result (fallback or normal)
                     cat = res.get("category", "KH√ÅC").upper()
                     if cat not in categorized_results: cat = "KH√ÅC"
                     categorized_results[cat].append(res["text"])
                 else:
                     # It is a failure
                     failed_articles.append(res)
            else:
                 cat = res.get("category", "KH√ÅC").upper()
                 if cat not in categorized_results: cat = "KH√ÅC"
                 categorized_results[cat].append(res["text"])

        # Format Markdown
        final_summary = f"# TIN T·ª®C T·ªîNG H·ª¢P ({datetime.now().strftime('%d/%m/%Y')})\n\n"
        
        priority_order = ["KINH T·∫æ", "T√ÄI CH√çNH", "X√É H·ªòI", "PH√ÅP LU·∫¨T", "TH·∫æ GI·ªöI", "KH√ÅC"]
        
        for cat in priority_order:
            articles_list = categorized_results.get(cat, [])
            if articles_list:
                final_summary += f"## {cat}\n\n"
                for idx, article_text in enumerate(articles_list, 1):
                    final_summary += f"{article_text}\n\n"
                final_summary += "---\n\n"
        
        if failed_articles:
             final_summary += f"### ‚ö†Ô∏è Kh√¥ng th·ªÉ t√≥m t·∫Øt ({len(failed_articles)} b√†i)\n"
             # ... error details ...
        
        yield {
            "type": "complete", 
            "summary": final_summary
        }
        """
        Fetch and summarize articles in parallel, then group by category.
        """
        if articles_metadata is None:
            articles_metadata = {}
            
        print(f"üöÄ Starting parallel summarization for {len(urls)} articles...")
        
        # Create tasks for each URL
        tasks = []
        total = len(urls)
        completed_count = 0
        
        async def track_progress(url, metadata, api_key):
            nonlocal completed_count
            # Notify start
            if progress_callback:
                try:
                    await progress_callback(completed_count, total, url, "processing")
                except Exception as e:
                    print(f"Error in progress callback: {e}")
                
            result = await self._process_single_article(url, metadata, api_key)
            
            completed_count += 1
            # Notify completion (success or fail)
            if progress_callback:
                try:
                    await progress_callback(completed_count, total, url, "completed")
                except Exception as e:
                    print(f"Error in progress callback: {e}")
                
            return result

        for url in urls:
            clean_url = url.strip().rstrip('/')
            metadata = articles_metadata.get(clean_url, {})
            # If not found, try original just in case
            if not metadata:
                metadata = articles_metadata.get(url, {})
            
            # Fallback: Infer category if missing
            if not metadata or not metadata.get('category'):
                inferred_category = rss_fetcher._extract_category_from_url(url)
                if not metadata:
                    metadata = {'source': None, 'title': None}
                metadata['category'] = inferred_category
                
            task = track_progress(url, metadata, api_key)
            tasks.append(task)
        
        # Process in chunks to prevent overload (BATCH_SIZE = 5)
        BATCH_SIZE = 5
        all_results = []
        
        for i in range(0, len(tasks), BATCH_SIZE):
            batch_tasks = tasks[i:i + BATCH_SIZE]
            print(f"üîÑ Processing batch {i//BATCH_SIZE + 1}/{(len(tasks) + BATCH_SIZE - 1)//BATCH_SIZE}...")
            
            # Run batch concurrently
            batch_results = await asyncio.gather(*batch_tasks)
            all_results.extend(batch_results)
            
            # Optional: Add delay between batches if needed to cool down rate limits
            if i + BATCH_SIZE < len(tasks):
                await asyncio.sleep(2) 
        
        results = all_results
            
        # Collect results
        category_groups = {}
        failed_articles = []
        
        valid_count = 0
        for i, res in enumerate(results):
            if res and "text" in res:
                # Success case
                cat = res.get("category", "KH√ÅC")
                if cat not in category_groups:
                    category_groups[cat] = []
                category_groups[cat].append(res["text"])
                valid_count += 1
            elif res and "error" in res:
                # Error case
                url = urls[i] if i < len(urls) else "Unknown URL"
                failed_articles.append(f"- [{url}]({url}): {res['error']}")
            else:
                # None or unknown
                pass
        
        print(f"‚úÖ Completed. Valid: {valid_count}/{len(urls)}, Failed: {len(failed_articles)}")
        
        # Build final Markdown
        final_markdown_parts = []
        
        # 1. Success Groups
        sorted_categories = sorted(category_groups.keys())
        for i, cat in enumerate(sorted_categories, 1):
            articles = category_groups[cat]
            final_markdown_parts.append(f"## {i}. Chuy√™n m·ª•c {cat} ({len(articles)} b√†i)")
            final_markdown_parts.extend(articles)
            final_markdown_parts.append("") 
            
        # 2. Failed List (if any)
        if failed_articles:
            final_markdown_parts.append("---")
            final_markdown_parts.append(f"### ‚ö†Ô∏è Kh√¥ng th·ªÉ t√≥m t·∫Øt ({len(failed_articles)} b√†i)")
            final_markdown_parts.append("\n".join(failed_articles))
        
        return "\n\n".join(final_markdown_parts)

    async def _process_single_article(self, url: str, metadata: dict, api_key: str) -> dict:
        """
        Fetch and summarize a single article with concurrency control.
        Returns dict: {"category": str, "text": str} or None
        """
        async with self.semaphore:


            try:
                # Prepare metadata first for fallback
                source = metadata.get('source', 'Ngu·ªìn Kh√°c')
                category = metadata.get('category', 'TIN T·ª®C') 
                title = metadata.get('title', 'Ti√™u ƒë·ªÅ b√†i vi·∫øt')
                
                # Fallback text generator
                def get_fallback_summary(reason=""):
                    return (
                        f"### [{title}]({url})\n"
                        f"**Ngu·ªìn:** {source}\n\n"
                        f"*(B√†i vi·∫øt c√≥ n·ªôi dung ng·∫Øn ho·∫∑c h√¨nh ·∫£nh, vui l√≤ng xem chi ti·∫øt t·∫°i ƒë∆∞·ªùng d·∫´n)*"
                    )

                # 1. Fetch Content
                await asyncio.sleep(0.5) 
                
                content = None
                fetch_retries = 3
                for attempt in range(fetch_retries):
                    try:
                        raw_html = await secure_fetcher.fetch_rss(url)
                        # Try to extract
                        soup = BeautifulSoup(raw_html, 'html.parser')
                        # Remove script/style
                        for script in soup(["script", "style", "nav", "footer", "header", "iframe", "noscript"]):
                            script.decompose()
                            
                        # Selectors logic (inlined or called)
                        # We call the helper
                        extracted = self._extract_content(raw_html, limit=15000)
                        
                        if extracted and len(extracted) >= 200:
                            content = extracted
                            break
                        else:
                            # If extraction failed/short, wait and retry? 
                            # Actually if content is short, retrying might not help if it's the article nature.
                            # But we retry for network issues.
                            # For now, let's just wait.
                            if attempt < fetch_retries - 1:
                                await asyncio.sleep(2 * (attempt + 1))
                    except Exception as e:
                        print(f"   ‚ùå Fetch error attempt {attempt+1} for {url}: {e}")
                        await asyncio.sleep(2 * (attempt + 1))
                
                # If still no content or content too short
                if not content or len(content) < 200:
                    print(f"‚ö†Ô∏è Content short/empty for {url}, returning fallback.")
                    return {
                        "category": category.upper(),
                        "text": get_fallback_summary()
                    }

                # 2. Prepare Prompt
                prompt = SINGLE_ARTICLE_SUMMARIZE_PROMPT.format(
                    title=title,
                    content=content,
                    source=source,
                    category=category,
                    url=url
                )

                # 3. Generate Summary with Gemini (with Retry)
                retries = 3
                for attempt in range(retries):
                    try:
                        summary = await gemini_client.async_generate_content(
                            prompt=prompt,
                            model_name="gemini-2.0-flash",
                            temperature=0.2,
                            max_tokens=1000,
                            api_key=api_key
                        )
                        
                        print(f"   ‚úÖ Summarized: {url[:30]}...")
                        
                        return {
                            "category": category.upper(), # Normalize category
                            "text": summary
                        }
                    except Exception as e:
                        if "429" in str(e) or "Resource exhausted" in str(e):
                            wait_time = (attempt + 1) * 2
                            print(f"   ‚è≥ Rate limit Gemini for {url[:20]}... Retrying in {wait_time}s...")
                            await asyncio.sleep(wait_time)
                        else:
                            print(f"   ‚ùå Error generating summary for {url}: {str(e)}")
                            break # Don't retry generic errors
                
                return None

            except Exception as e:
                print(f"‚ùå Error processing {url}: {str(e)}")
                return {"error": f"L·ªói x·ª≠ l√Ω: {str(e)}"}

    def _extract_content(self, html: str, limit: int = 8000) -> str:
        """
        Extract main content from HTML using BeautifulSoup
        """
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # Remove script and style elements
            for script in soup(["script", "style", "nav", "footer", "header", "iframe", "noscript"]):
                script.decompose()
            
            # Attempt to find main content areas for cleaner text
            # Common selectors for news sites
            main_content = None
            selectors = [
                # Specific Selectors (High Priority)
                '.entry', # HanoiMoi (Verified)
                '.b-maincontent', # HanoiMoi
                '.details__content', # Lao Dong
                '.cms-body', # VietnamPlus
                '#article-body', # TienPhong, SGGP
                '.article-body', 
                '.fck_detail', # VnExpress/DanTri
                '.post-content', 
                '[role="main"]', 
                'article', 
                
                # Generic Selectors (Low Priority - Use with caution)
                '.article-content', 
                '.content-detail', 
                '.detail-content', 
                '.post_content',
                '.body-content',
                '#content', 
                '.content' 
            ]
            
            for selector in selectors:
                found = soup.select_one(selector)
                if found:
                    temp_text = found.get_text(separator=' ', strip=True)
                    if len(temp_text) > 200:
                        main_content = found
                        break
            
            target = main_content if main_content else soup

            # Get text
            text = target.get_text(separator=' ', strip=True)
            
            # Clean up whitespace
            text = ' '.join(text.split())
            
            # Limit content
            return text[:limit]
        except Exception:
            return ""

summarizer = Summarizer()
