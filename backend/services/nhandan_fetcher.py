"""
Nhan Dan Fetcher Service
L·∫•y RSS t·ª´ B√°o Nh√¢n D√¢n v√† ƒë·ªëi chi·∫øu n·ªôi dung
"""

import asyncio
from typing import List, Dict, Optional
from datetime import datetime, timedelta
from services.secure_fetcher import secure_fetcher
import feedparser
import json
from services.fast_gemini import fast_gemini


class NhanDanFetcher:
    def __init__(self):
        # RSS URLs cho c√°c chuy√™n m·ª•c ch√≠nh
        self.rss_urls = {
            "KINH T·∫æ": "https://nhandan.vn/rss/kinhte-1185.rss",
            "PH√ÅP LU·∫¨T": "https://nhandan.vn/rss/phapluat-1287.rss",
            "X√É H·ªòI": "https://nhandan.vn/rss/xahoi-1211.rss",
            "TH·∫æ GI·ªöI": "https://nhandan.vn/rss/thegioi-1231.rss",
        }
        
        # Cache headlines (in-memory)
        self.cached_headlines: List[Dict] = []
        self.last_fetch_time: Optional[datetime] = None
        self.cache_duration = timedelta(hours=1)  # Refresh m·ªói 1 gi·ªù
        
    async def setup_background_fetch(self):
        """
        Fetch RSS feeds t·ª´ B√°o Nh√¢n D√¢n v√† cache
        """
        print("üîÑ Fetching B√°o Nh√¢n D√¢n RSS...")
        all_articles = []
        
        for category, rss_url in self.rss_urls.items():
            try:
                # S·ª≠ d·ª•ng SecureRSSFetcher cho c√°c trang c√≥ ch·ªëng bot
                rss_content = await secure_fetcher.fetch_rss(rss_url)
                feed = feedparser.parse(rss_content)
                
                for entry in feed.entries[:20]:  # L·∫•y 20 b√†i m·ªõi nh·∫•t m·ªói chuy√™n m·ª•c
                    all_articles.append({
                        "title": entry.get('title', ''),
                        "link": entry.get('link', ''),
                        "category": category,
                        "published": entry.get('published', '')
                    })
            except Exception as e:
                print(f"‚ö†Ô∏è Error fetching Nhan Dan RSS for {category}: {e}")
        
        self.cached_headlines = all_articles
        self.last_fetch_time = datetime.now()
        print(f"‚úÖ Cached {len(all_articles)} articles from B√°o Nh√¢n D√¢n")
        
        return all_articles
    
    async def ensure_cache_fresh(self):
        """ƒê·∫£m b·∫£o cache c√≤n m·ªõi (t·ª± ƒë·ªông refresh n·∫øu c·∫ßn)"""
        if not self.cached_headlines or not self.last_fetch_time:
            await self.setup_background_fetch()
        elif datetime.now() - self.last_fetch_time > self.cache_duration:
            await self.setup_background_fetch()
    
    async def check_official_coverage(
        self, 
        articles: List[Dict],
        api_key: str
    ) -> List[Dict]:
        """
        Ki·ªÉm tra xem c√°c b√†i vi·∫øt c√≥ ƒë∆∞·ª£c B√°o Nh√¢n D√¢n ƒëƒÉng kh√¥ng
        S·ª≠ d·ª•ng batch processing theo category ƒë·ªÉ t·ªëi ∆∞u t·ªëc ƒë·ªô
        
        Args:
            articles: Danh s√°ch b√†i vi·∫øt c·∫ßn ki·ªÉm tra
            api_key: Gemini API key
            
        Returns:
            Danh s√°ch b√†i vi·∫øt ƒë√£ ƒë∆∞·ª£c g·∫Øn th√™m field 'official_source_link'
        """
        # Skip if no API key (avoid slow processing)
        if not api_key:
            print("‚ö†Ô∏è No API key - skipping Nhan Dan verification")
            for article in articles:
                article['official_source_link'] = None
            return articles
        
        await self.ensure_cache_fresh()
        
        if not self.cached_headlines:
            print("‚ö†Ô∏è No Nhan Dan cache available")
            for article in articles:
                article['official_source_link'] = None
            return articles
        
        # Group articles by category
        MAX_ARTICLES_PER_CATEGORY = 70  # Limit to prevent timeout
        articles_by_category = {}
        for article in articles:
            category = article.get('category', 'KH√ÅC')
            if category not in articles_by_category:
                articles_by_category[category] = []
            articles_by_category[category].append(article)
        
        print(f"üîç Checking Nhan Dan coverage for {len(articles)} articles across {len(articles_by_category)} categories")
        
        # Process each category in parallel (with limit)
        tasks = []
        for category, category_articles in articles_by_category.items():
            if category in self.rss_urls:
                # Limit articles per category
                if len(category_articles) > MAX_ARTICLES_PER_CATEGORY:
                    print(f"‚ö†Ô∏è Category {category} has {len(category_articles)} articles, limiting to {MAX_ARTICLES_PER_CATEGORY} for Nhan Dan check")
                    category_articles = category_articles[:MAX_ARTICLES_PER_CATEGORY]
                
                task = self._check_category_batch(category, category_articles, api_key)
                tasks.append(task)
            else:
                # Mark as not applicable
                for article in category_articles:
                    article['official_source_link'] = None
        
        # Run all category checks in parallel
        if tasks:
            await asyncio.gather(*tasks)
        
        return articles
    
    async def _check_category_batch(
        self,
        category: str,
        articles: List[Dict],
        api_key: str
    ):
        """
        Check m·ªôt batch articles c√πng category v·ªõi Nhan Dan headlines
        """
        # Get Nhan Dan headlines for this category
        category_headlines = [
            h for h in self.cached_headlines 
            if h['category'] == category
        ]
        
        if not category_headlines:
            for article in articles:
                article['official_source_link'] = None
            return
        
        # Batch check: Send all articles at once to Gemini
        await self._batch_semantic_match(articles, category_headlines, api_key)
    
    async def _batch_semantic_match(
        self,
        articles: List[Dict],
        nhan_dan_headlines: List[Dict],
        api_key: str
    ):
        """
        Batch semantic matching: Check nhi·ªÅu b√†i c√πng l√∫c
        """
        # Prepare batch data
        # Prepare batch data with XML tags
        articles_formatted = "\n".join([
            f"{i}. {a['title']}"
            for i, a in enumerate(articles)
        ])
        
        headlines_formatted = "\n".join([
            f"- {h['title']} | URL: {h['link']}"
            for h in nhan_dan_headlines[:20]  # Increased limit slightly
        ])
        
        prompt = f"""# Role
B·∫°n l√† m·ªôt AI News Aggregator Engineer chuy√™n nghi·ªáp. Nhi·ªám v·ª• c·ªßa b·∫°n l√† ƒë·ªëi chi·∫øu danh s√°ch c√°c "Tin t·ª©c m·ªõi" (Input) v·ªõi "C∆° s·ªü d·ªØ li·ªáu b√°o ch√≠" (Reference) ƒë·ªÉ t√¨m ra c√°c b√†i vi·∫øt tr√πng l·∫∑p v·ªÅ m·∫∑t s·ª± ki·ªán.

# Input Data
T√¥i s·∫Ω cung c·∫•p cho b·∫°n 2 danh s√°ch ƒë∆∞·ª£c bao b·ªçc b·ªüi th·∫ª XML:
1. <input_articles>: Danh s√°ch c√°c ti√™u ƒë·ªÅ c·∫ßn ki·ªÉm tra (ƒë√£ ƒë∆∞·ª£c ƒë√°nh ID).
2. <reference_database>: Danh s√°ch c√°c b√†i b√°o g·ªëc (g·ªìm Ti√™u ƒë·ªÅ v√† URL).

# Instruction (H∆∞·ªõng d·∫´n x·ª≠ l√Ω)
H√£y th·ª±c hi·ªán t·ª´ng b∆∞·ªõc suy lu·∫≠n sau:

B∆∞·ªõc 1: Tr√≠ch xu·∫•t th·ª±c th·ªÉ (Entity Extraction)
V·ªõi m·ªói b√†i trong <input_articles>, h√£y x√°c ƒë·ªãnh: Ai (Who), C√°i g√¨ (What), ·ªû ƒë√¢u (Where), Con s·ªë th∆∞∆°ng vong/thi·ªát h·∫°i (Numbers).

B∆∞·ªõc 2: So kh·ªõp (Matching Logic)
So s√°nh c√°c th·ª±c th·ªÉ tr√™n v·ªõi <reference_database>.
- Quy t·∫Øc kh·ªõp: Hai b√†i vi·∫øt ch·ªâ ƒë∆∞·ª£c coi l√† MATCH khi ch√∫ng n√≥i v·ªÅ C√ôNG M·ªòT S·ª∞ KI·ªÜN C·ª§ TH·ªÇ (Same specific event).
- Quy t·∫Øc lo·∫°i tr·ª´: 
  + C√πng ch·ªß ƒë·ªÅ nh∆∞ng kh√°c g√≥c ƒë·ªô -> KH√îNG KH·ªöP (V√≠ d·ª•: "Nga t√°i thi·∫øt Syria" kh√°c v·ªõi "H·ªçp LHQ v·ªÅ h√≤a b√¨nh Syria").
  + C√πng ƒë·ªãa ƒëi·ªÉm nh∆∞ng kh√°c s·ª± ki·ªán -> KH√îNG KH·ªöP.
  + N·∫øu nghi ng·ªù ho·∫∑c th√¥ng tin qu√° chung chung -> Tr·∫£ v·ªÅ null.

B∆∞·ªõc 3: Output JSON
Tr·∫£ v·ªÅ k·∫øt qu·∫£ d∆∞·ªõi d·∫°ng JSON Array, kh√¥ng k√®m l·ªùi gi·∫£i th√≠ch n√†o kh√°c ngo√†i JSON.

# Output Format
[
  {{
    "article_index": 0, 
    "matched_link": "URL_t·ª´_reference_list" (ho·∫∑c null n·∫øu kh√¥ng t√¨m th·∫•y)
  }},
  ...
]

# Data
<input_articles>
{articles_formatted}
</input_articles>

<reference_database>
{headlines_formatted}
</reference_database>
"""

        try:
            response = await fast_gemini.generate_content(
                prompt=prompt,
                model_name="gemini-2.0-flash",
                temperature=0.2,
                max_tokens=1024,
                api_key=api_key
            )
            
            # Parse response
            response_text = response.strip()
            if "```json" in response_text:
                response_text = response_text.split("```json")[1].split("```")[0].strip()
            elif "```" in response_text:
                response_text = response_text.split("```")[1].split("```")[0].strip()
            
            results = json.loads(response_text)
            
            # Apply results to articles
            for result in results:
                idx = result.get('article_index')
                if 0 <= idx < len(articles):
                    articles[idx]['official_source_link'] = result.get('matched_link')
            
        except Exception as e:
            print(f"‚ö†Ô∏è Batch semantic match error: {e}")
            # Fallback: mark all as not matched
            for article in articles:
                if 'official_source_link' not in article:
                    article['official_source_link'] = None
    
    async def _semantic_match(
        self, 
        article: Dict, 
        nhan_dan_headlines: List[Dict],
        api_key: str
    ) -> Optional[str]:
        """
        S·ª≠ d·ª•ng Gemini ƒë·ªÉ ki·ªÉm tra xem b√†i vi·∫øt c√≥ kh·ªõp v·ªõi b√†i n√†o c·ªßa Nh√¢n D√¢n kh√¥ng
        """
        # Prepare data
        headlines_text = "\n".join([
            f"{i+1}. {h['title']} ({h['link']})"
            for i, h in enumerate(nhan_dan_headlines[:10])  # Limit to 10 for token efficiency
        ])
        
        prompt = f"""B·∫°n l√† chuy√™n gia ph√¢n t√≠ch tin t·ª©c. H√£y ki·ªÉm tra xem b√†i vi·∫øt d∆∞·ªõi ƒë√¢y c√≥ c√πng n·ªôi dung v·ªõi b√†i n√†o trong danh s√°ch B√°o Nh√¢n D√¢n kh√¥ng.

B√†i vi·∫øt c·∫ßn ki·ªÉm tra:
Ti√™u ƒë·ªÅ: {article['title']}

Danh s√°ch B√°o Nh√¢n D√¢n:
{headlines_text}

Tr·∫£ v·ªÅ JSON:
{{
  "is_match": true/false,
  "matched_link": "url n·∫øu kh·ªõp, null n·∫øu kh√¥ng"
}}

L∆∞u √Ω: Ch·ªâ tr·∫£ v·ªÅ true n·∫øu ch·∫Øc ch·∫Øn 2 b√†i vi·∫øt v·ªÅ c√πng s·ª± ki·ªán.
"""

        try:
            response = await fast_gemini.generate_content(
                prompt=prompt,
                model_name="gemini-2.0-flash",
                temperature=0.2,
                max_tokens=256,
                api_key=api_key
            )
            
            # Parse response
            response_text = response.strip()
            if "```json" in response_text:
                response_text = response_text.split("```json")[1].split("```")[0].strip()
            elif "```" in response_text:
                response_text = response_text.split("```")[1].split("```")[0].strip()
            
            result = json.loads(response_text)
            
            if result.get('is_match'):
                return result.get('matched_link')
            
        except Exception as e:
            print(f"‚ö†Ô∏è Semantic match error: {e}")
        
        return None


# Singleton
nhandan_fetcher = NhanDanFetcher()
